## Stats500 Homework 6
#### *Di Lu, Nov.8, 2017*

### Q1. Based on Chapter 4, Problem 5(p. 97) 
#### 1. Using the sat data, fit a model with total as the response and takers, ratio, salary and expend as predictors

(1) Ordinary least squares
At 5% significance level, we see takers are significant(p value = 2.607e-16), coefficient is -2.90448. R-square of OLS is 0.82. Residual SE = 32.70.
```{r}
library(faraway)
attach(sat)
# Ordinary least squares
lmod_ols <- lm(total~takers+ratio+salary+expend)
sumary(lmod_ols)
```

(2) Least absolute deviations
The 95% confidence of takers doesnot include 0, means that at 5% significance level, takers is significant, estimate of coefficient is -3.13961.
```{r}
library(quantreg)
lmod_lad <- rq(total~takers+ratio+salary+expend)
summary(lmod_lad)
```

(3) Huber's robust regression
At 5% significance level, we see takers are significant(t value -13.6470, abs value> 2), coefficient is -2.9778. R-square of OLS is 0.82. Residual SE = 25.58.
```{r}
library(MASS)
lmod_huber <- rlm(total~takers+ratio+salary+expend)
summary(lmod_huber)
```

(4) Least trimmed squares
We use bootstrap to get confidence interval in LTS. The 95% confidence of takers doesnot include 0, means that at 5% significance level, takers is significant, estimate of coefficient is -3.1655. Ratio is marginal significant.
```{r}
library(MASS)
lmod_lts <- ltsreg(total~takers+ratio+salary+expend, nsamp = 'exact')
round(lmod_lts$coef,4)
# Bootstrap to get confidence interval of LTS method
x <- sat[,c(4,2,3,1)]
bcoef <- matrix(0, nrow = 1000, ncol = 5)
for (i in 1:1000){
  newy <- lmod_lts$fit+lmod_lts$residuals[sample(50,rep=T)]
  bcoef[i,] <- ltsreg(x,newy,nsamp = "best")$coef
}
# 95% CI for parameters
colnames(bcoef) <- names(coef(lmod_lts))
apply(bcoef,2,function(x) quantile(x,c(0.025,0.975)))
```
By ploting results of bootstrap as density and CI marked as dashed line, we can see at 5% significance level, takers is significant.
```{r}
library(ggplot2)
bcoef <- data.frame(bcoef)
library(gridExtra)
grid.arrange(plot1, plot2, plot3, plot4, nrow=2, ncol=2)
plot1<-ggplot(bcoef,aes(x=takers))+geom_density() + geom_vline(xintercept = c(-3.75,-2.41),linetype="dashed")
plot2<-ggplot(bcoef,aes(x=ratio))+geom_density() + geom_vline(xintercept = c(-19.68,0.10),linetype="dashed")
plot3<-ggplot(bcoef,aes(x=salary))+geom_density() + geom_vline(xintercept = c(-5.24,9.05),linetype="dashed")
plot4<-ggplot(bcoef,aes(x=expend))+geom_density() + geom_vline(xintercept = c(-18.97,42.46),linetype="dashed")
```

(5) Comparison:
From the four regression methods above, we can tell the results agree with each other. Takers is significant at 5% level and coefficient is around -3.0. Other predictors are not significant, Ratio is marginally significant in huber's method and LTS.

#### 2. Use diagnostic methods to detect any outliers or inuential points. Remove these points and then use least squares. Compare the results with part (a), and the other robust regression results.
(1) Diagnostics
* First we check error is approximatey normal distributed. 
* Second, largest residual point is "West Virginia", we will look out for it. 
* Third, we use half-normal plot to help identify leverage points. "Utah" and "California" have largest leverage. Use jackknife residuals and the Bonferroni correction to check if we can find high leverage points. Jacknife of largest leverage point is -3.12, abosulte value is not bigger than that of Bonferroni criteria(qt(0.05/(50*2),44) = -3.52). So we cannot reject it is not a high leverage point.
* Fourth, Cook's distance combine the effect of leverage and residual. "Utah" stands out in Cook's distance.
* To sum up, we take "Utah" as a influential point and remove it.
```{r}
par(mfrow=c(2,2))
# Check normality
qqnorm(lmod_ols$residual, ylab="Residuals")
qqline(lmod_ols$residual)
plot(lmod_ols$fitted.values,lmod_ols$residuals,xlab="fitted values", ylab="Residuals", main = "Residuals")
outlier <- which.max(abs(lmod_ols$residuals))
text(lmod_ols$fitted.values[outlier],lmod_ols$residuals[outlier],labels = row.names(sat)[outlier])
# A half-normal plot will help identify leverage points.
halfnorm(lm.influence(lmod_ols)$hat,labs=row.names(sat),ylab="Leverages")
# use jackknife residuals and the Bonferroni correction to see any outliers
jack<-rstudent(lmod_ols)
jack[order(abs(jack),decreasing=TRUE)][1:5]
qt(0.05/(50*2),44)
# Cook distance
cook = cooks.distance(lmod_ols)
halfnorm(cook,labs=row.names(sat),ylab="Cook¡¯s distance")
```

(2) remove obs. Utah and rerun regression
The OLS regression without "Utah" shows that at 5% significance level, takers is significant(p value = <2e-16) with coefficient -2.93080. This is similar to previous result and that of other robust methods. At 5% significance level, ratio is significant(p value = 0.0310) with coefficient -7.63. This changes the sigficance compared to OLS on whole data, and is closer to the result of the Huber's method and LTS with bootstrap. 
So we can conclude that after running diagnostic and removing outlier, the result of OLS is similar to that of robust methods.
```{r}
lmod_ols_2 <- lm(total~takers+ratio+salary+expend,subset = (row.names(sat)!="Utah"))
sumary(lmod_ols_2)
```

